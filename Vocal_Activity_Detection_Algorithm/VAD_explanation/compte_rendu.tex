\documentclass[a4paper,11pt]{article}
\usepackage[latin1]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{anysize}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{mon_theme}
\usepackage{moreverb}
\usepackage{titlesec}
\usepackage{wrapfig,, blindtext}
\usepackage{subcaption}
\usepackage[export]{adjustbox}
\usepackage[squaren,Gray]{SIunits}
\usepackage{array,multirow,makecell}
\usepackage{sidecap}
\usepackage{colortbl}
\usepackage{pdfpages}
\usepackage{lastpage}
\usepackage{hyperref}
\usepackage{amssymb, amsmath}
\usepackage{multirow}
\usepackage{titlesec}
\usepackage[nottoc, notlof, notlot]{tocbibind}
\usepackage{url}
\usepackage{array}
\usepackage[dvipsnames]{xcolor}



\title{}
\date{}
\author{}
\definecolor{ROUGEENSEA}{rgb}{0.75,0.01,0.25}
\definecolor{ulaval_rouge}{RGB}{255,0,0}
\definecolor{ulaval_jaune}{RGB}{254,203,47}
\begin{document}
 
\pageDeGarde{\textsc{Vocal Activity Algorithm }}{\Huge{\textsc{ Module commande de la parole}}}{{\large {\bfseries{\textsc{Development and integration of a voice command for users  with speech disabilities (dystrophy, paralysis, etc.) on the JACO robotic arm}}}}}{}
\newpage
\pageMATIERE{}
\renewcommand{\contentsname}{Table des matières}
\tableofcontents
\newpage


\pageTHEME
\setcounter{page}{1}
\section*{Résumé}
L'objectif de ce rapport est de confronter les performances de mon algorithme de Détection d'Activité de la Voix dans une trame audio (DAV ou Vocal Activity Detection en anglais). Le DAV est un algorithme  qui vise à extraite d'une trame audio tous les tronçons où il y a présence de voix et cherche à accélérer le processus de reconnaissance de la parole. En effet, en ne se concentrant que sur les tronçons de voix, on ne cherche pas à reconnaitre des silences entre des mots, ... mais que des mots, phonèmes, ... (cela dépend de la manière dont est réalisé l'algorithme de reconnaissance de la parole). 
\paragraph{} Actuellement, plusieurs méthodes sont utilisées pour réaliser cette opération de détection de flux de parole dans différents domaines (temporels et fréquentiels). Le domaine utilisé va bien sûr impacter sur la rapidité de l'algorithme. Le domaine temporel ne nécessitant pas de calcul complexe, il rendra notre algorithme plus rapide néanmoins il sera plus sensible au bruit. Le domaine fréquentiel est quant à lui moins sensible au bruit mais est plus gourmand en calcul. Les méthodes les plus connues dans le domaine temporel sont l'énergie à court terme combinée aux taux de passage à zéro (Short time Energy and Zero Crossing Rate) et dans le domaine fréquentiel : l'Entropie (Spectral Entropy), le Spectral Centroïd  (qui est une sorte d'indicateur du "centre de masse" du spectre) mais aussi les Mel Feature Cepstrum Coefficents (MFCC).
\paragraph{} Mon partis pris a été de choisir de travailler dans le domaine fréquentiel essentiellement afin d'avoir un algorithme très robuste au bruit et de combiné l'Entropy au MFCC. De plus, même si mes deux méthodes sont très peu sensible au bruit, elles perdent un peu de leur immunité à partir d'un certain SNR (Signal Noise Ratio ou Rapport de Signal à Bruit en français). De fait, afin de pouvoir travailler dans des environnements assujettis à des SNR très faible, j'ai donc généré une fonction d'initialisation qui récupère l'empreinte du bruit environnant et s'en sers de vecteurs modèles. Les vecteurs extraits des deux méthodes sont donc comparés à ces modèles de bruit par distance euclidienne ou par corrélation pour chaque trame. Les vecteurs modèles sont mis à jour à chaque trame pour suivre l'évolution des éléments en entrées.
\paragraph{} L'étude comparative de mon modèle se fera par confrontation à quatre modèles et avec des contraintes de bruits. 
\newpage
\section{Introduction}
Ce rapport s'organise de la façon suivante. Tout d'abord quelques notions théoriques nécessaires à la bonne compréhension du sujet seront introduites. Un état de l'art non exhaustif de la détection de l'activité vocale sera présenté. Ensuite, les différents travaux réalisés seront décrits. Ces travaux reposent essentiellement sur les Mel Feature Cepstrum Coefficient (MFCC) et l'Entropie, dans l'objectif de rejeter toutes les trames audio non utiles. Les performances du VAD des méthodes proposées seront exposées puis comparées à des méthodes de références. Ce sera alors l'occasion de soulever des difficultés liées aux différentes méthodes mais aussi des améliorations qui pourraient y être éventuellement apportés. 
\newpage
\section{Notions théoriques}
\subsection{Traitement à court-terme du signal vocal}
On peut assez facilement constater que la forme d'onde d'un signal vocal met en évidence son caractère non stationnaire. Étant donné son caractère non stationnaire, un étude à long terme ou globale est généralement inefficaces. L'hypothèse la plus utilisée dans le traitement de la parole est le fait que les propriétés du signal vocal change lentement dans le temps \cite{ref}. Cette hypothèse conduit à un traitement à court terme. De plus, l'analyse à court terme permet de respecter le caractère temps réel que nous souhaitons donner à notre algorithme par la suite.
\subsection{Méthodes dans le domaine temporel}
Les méthodes ci-dessous sont une liste non exhaustive des méthodes temporelles utilisées. Elles sont néanmoins celle le plus couramment utilisées. Les définitions de chaque méthode est extraite de l'article suivant \cite{temp_dom}
\subsubsection{Énergie à court terme}
L'énergie court-terme est un paramètre qui reflète les variations d'amplitude dans le signal vocal. Elle fut un de premiers paramètres utilisés dans la détection d'activité vocale. Elle permet de classifier les trames voisées et non voisées. Elle est définit de la manière suivante :\\
\begin{equation}
	E_n=\sum_{k=n-N+1}^{n}[x(k).w(n-k)]^2
\end{equation}
où $w(n-k)$ est la fenêtre d'analyse, $n$ est l'échantillon sur lequel est centré la fenêtre d'analyse et $N$ la taille de la fenêtre d'analyse.
\subsubsection{Taux de passage par zéro (Zero Crossing Rate, ZRC)}
Le ZRC compte le nombre de passage par zéro du signal. Les trames de voisées ont un taux de passage à zéro faible alors l'inverse des trames non voisées qui ont un ZRC élevé. On peut définir le ZRC de la manière suivante :
\begin{equation}
	Z_n= \sum_{m=-\infty}^{infty} |sgn[x(m)]-sgn[x(m -1)]lw(n-m)
\end{equation}
où 
\begin{equation}
	sgn[x(n)]=
     \begin{cases}
        1 & \text{ $x(n)\geq 0$ } \\
        -1  & \text{ $x(n)<0$}
     \end{cases}
\end{equation}
et
\begin{equation}
	w(n)=
     \begin{cases}
        1/(2N) & \text{$0 \leq n \leq N-1$ } \\
        0 & \text{otherwise}
     \end{cases}
\end{equation}
\subsection{Domaine fréquentiel}
\subsubsection{Entropie Spectrale}
Nous ne pouvons pas parler d'Entropie spectrale sans parler auparavant de l'entropie introduite par Claude Shannon dans sa théorie de l'information. Cette dernière permet de mesurer la quantité d'information contenue dans un signal aléatoire. Elle est définit de la manière suivante :
\begin{equation}
H(x)=\sum_{k}p(x_k).log_2[P(x_k)]
\end{equation}
où $x={x_k}_{0 \leq k\leq N-1}$ est une série temporelle, fréquentielle ou autre et où $P(k)$ est la probabilité d'un certain état $x_k$. 
\paragraph{Travaux de Shen J, Hung J et Lee J \cite{entropie}}
Shen J., Hung J. et Lee J. ont été les premiers a utilisé l'entropie dans le cadre de la détection de la parole. Ils ont démontré que l'entropie d'un flux de parole diffère de celui d'un flux sans parole. En effet, leur étude a montré que la structure de la voix reflète d'une organisation que l'on ne retrouve pas dans la structure spectrale du silence.
\paragraph{L'entropie spectrale}
L'entropie de Shannon mesurant la quantité d'information contenue dans un signal aléatoire, il est plus d'usage d'utiliser l'entropie spectrale : la structure harmonique d'un segment de voix n'apparaissant que dans son spectrogramme. L'entropie spectrale est définit tout d'abord par la transformée de Fourier à court terme (Short time Fourier Transform) :
\begin{equation}
	S(k,l)=\sum_{n=1}^N h(n)s(n-l)\exp{\frac{-j2\pi kn}{N}} ~\text{avec} ~0 \leq k \leq K-1 \text{et} ~K=N
\end{equation}
$S(k,l)$ représente l'amplitude de la $k^{ième}$ composante fréquentielle, pour la $l^{ième}$ trame d'analyse. $s(n)$ est l'amplitude du signal au temps $n$. $N$, le nombre de points considérés pour la transformée de Fourier. $h(n)$ est la fenêtre d'analyse (généralement, une fenêtre de hamming).\\
\paragraph{Energie spectrale} On définit l'énergie spectrale de la manière suivante :
\begin{equation}
S_{energy}(k,l)=|S(k,l)|^2~\text{avec}~1 \leq k \leq \frac{N}{2}
\end{equation}
\paragraph{P(k,l)} La probabilité associée à chaque composante spectrale est obtenue en normalisant :
\begin{equation}
P(k,l)=\frac{S_{energy}(k,l)}{\sum_{i=1}^{\frac{N}{2}} S_{energy}(i,l) }~\text{avec}~1 \leq i \leq \frac{N}{2}~\text{et}\sum_iP(i,l)=1~\text{pour tout $l$}.
\end{equation}
\paragraph{Entropie Spectrale} L'entropie spectrale s'appuie sur l'entropie de Shannon :
\begin{equation}
	H(l)=\sum_{i=1}^{\frac{N}{2}}P(i,l).\log_2|P(i,|)|
\end{equation}	
Il est plus souvent d'usage de considérer l'opposé de l'entropie pour obtenir des profils analogues à ceux de l'intensité.
\paragraph{Limitation} Une des limitations de l'entropie spectrale exposé par Ouzounov \cite{entropy} est que l'entropie d'une trame sans parole bruitée par un bruit blanc coloré peut-être équivalente à celle d'une trame avec parole mais bruitée.
\subsubsection{Spectral Centroïd}
Le "centroïde spectral" $C_i$ de la $i^{ième}$ trame est défini comme le centre de gravité de son spectre.
\begin{equation}
C_i=\frac{\sum_{k=1}^{N}(k+1)X_i(k)}{\sum_{k=1}^N X_i(k)}.
\end{equation}
$X_i(k)$ avec $k=1, ..., N$ sont les coefficients de la $i^{ième}$ courte trame de la transformée de Fourier discrète (DFT). $N$ est la longueur de la trame. Le centroïde spectral est une mesure de la position spectrale où des valeurs élevées correspondent à des sons plus brillant.\\
\\
Ces explications sont issues de l'article de Theodoros Giannakopoulos \cite{centroid}. Son algorithme et son étude seront utilisés par la suite pour comparer les résultats de notre algorithme.
\subsubsection{Mel Feature Cepstral Coefficients (MFCC)}
Les explications qui suivent sont issues et pour la plupart recopier du site $practicalcryptography.com$ \cite{MFCC}.

\paragraph{} Les Mel Features Cepstral Coefficients (MFCCs) sont des caractéristiques majoritairement utilisées en reconnaissance de parole automatique. Les MFCCS ont été introduites par Davis and Mermelstein dans les années 80, et ont été "l'état de l'art" depuis. Avant la mise en place des MFCC, Les coefficients de prédiction linéaire (LPC) et les coefficients cepstaux de prédilection linéaire (LPCC)  constituaient la caractéristique principale de reconnaissance automatique de la parole (ASR), en particulier avec les classificateurs HMM . 
\paragraph{Etape pour déterminer les MFCCs}
Pour déterminer les MFCCs, ll faut commencer, comme tout au long de notre étude, par sectionner le signal en courte trame se recouvrant les unes aux autres (overlapping). Pour chaque trame, on calcule l'estimation du périodogramme du spectre de puissance (en d'autres termes, on regarde la distribution de notre périodogramme). Ensuite, on applique un banc de filtres Mel aux spectres de puissances, somme de l'énergie dans chaque filtre. On prend le logarithme de toutes les énergies du banc de filtre. Puis la transformée en cosinus inverse (DCT). On ne garde que les 12 premiers coefficients en omettant le premier. 
\newpage
\section{Travaux réalisés}
Cette partie va aborder du travail réalisé sur l'algorithme de détection vocale (VAD). Une grande partie est inspirée de méthodes et modèles existants. En ce qui concerne le modèle de bruit de fond avec mise à jour à chaque trame, il est extrait des travaux Hongzhi Wang et Yuchao Xu, Meijing Li \cite{background}. J'ai donc transposé leur travaux à l'entropie afin de la rendre plus robuste au bruit. Nous avions parler d'une des limitations de l'entropie un peu plus que nous avons cherché à combler par cette méthode.
\subsection{Fonctionnement général de l'algorithme}
Avant de rentrer des explications plus approfondis sur les méthodes utilisées ainsi que leurs outils, nous allons expliquer globalement le fonctionnement de notre algorithme. \\
\paragraph{} Tout d'abord, une première fonction $recorder.m$ \cite{matlab} permet l'acquisition de donnée par la carte son de mon laptop et de sauvegarder des données dans des veccteurs. Deux acquisition sont réalisées : 
\begin{itemize}
	\item Acquisition du bruit environnant
	\item Acquisition d'une trame de voix durant 3s (le temps est ici arbitraire. D'autant plus, que par la suite nous chercherons à être en temps réel).
\end{itemize}
Une fois ces deux acquisitions opérées, nous utilisons celle de bruit de fond afin de générer un vecteur $MFCCs_{noise}$. Pour cela, nous "découpons" le signal acquis en plusieurs trames de $15 \milli \second$ avec un recouvrement de $10 \milli \second$ afin de respecter l'hypothèse du caractère stationnaire exprimé un peu plus haut. Ces trames (ou fenêtres) sont ensuite envoyées à un algorithme : $ short_time_Fourier_transform.m$ pour réaliser une transformée de Fourier (l'appellation de la fonction est un peu faussée car nous sommes déjà dans un cas de court terme). Enfin, on extrait les Mel Feature Cepstral Coefficients du signal de bruit de fond (toutes les trames) que nous moyennons. Ce vecteur est donc la moyenne des coefficients cepstraux du bruit (en considérant que seule du bruit de fond est présent dans cette acquisition). On opère similairement pour l'entropie, en calculant la valeur moyenne de l'entropie pour le signal de bruit de fond (on réalise la moyenne sur l'ensemble des trames). 
\paragraph{} Pour l'acquisition d'une trame de voix, on réalise aussi le "découpage" en trame avec chevauchement. Pour chaque trame, on calcule l'entropie et les coefficients cepstaux. On compare le vecteur de coefficients cepstraux et la valeur de l'entropie de chaque trame à respectivement, le vecteur de bruit de fond de la moyenne des coefficients cepstraux et à la valeur moyenne du bruit de fond de l'entropie. On utilise pour cela, respectivement, la corrélation et la distance euclidienne. 
\paragraph{} On applique ensuite des seuils pour chacune des deux variables évoluant au cours du temps afin d'être le plus pertinent possible. Chaque trame qui est au dessus du seuil est labellisée d'un "un" sinon elle l'est d'un "zéro". Le vecteur contenant les labels est ensuite envoyé à une fonction permettant de corriger les valeurs aberrantes (par exemple un "un" perdu au milieu de "zéro" et vis-versa. L'algorithme corrige jusqu'à plusieurs paquets de valeurs aberrantes en fonction des paramètres qu'on lui rentre en argument). Pour cela, on s'appuie du poids des labels au nombres paires entourant chaque label considéré. 
\paragraph{} Une fois l'opération de correction de label réalisée, nous n'avons plus qu'à extraire les segments (en ms et non en trame comme nous avions précédemment) en considérant nos labels de "un" comme de la voix et les autres comme du silence, du bruit et des parasites sur le signal. Enfin, un dernier algorithme tri les segments récupérés. En effet, dans la littérature, chaque phonème dure un certain temps (selon la langue, le plus petit dure un $20 ms$ et le plus long, souvent voisé, dure $100 ms$. Dans notre cas, on suppose que notre algorithme reconnait un ensemble de phonème, soit une syllabe. Dans ce cas, on considère donc qu'en dessous de $80 ms$, on rejette le segment considéré. LITTERATURE -REF ?
\paragraph{Finalité} Nous obtenons donc de l'acquisition d'un signal de trois secondes (choix arbitraires), un ensemble de segments contenant de la parole dans la majeur partie des cas (en effet, des bruits peuvent-être présents). Nous conclurons sur les manières d'améliorer notre algorithme. Néanmoins, cela ralentira notre algorithme sachant qu'il est déjà plus lent que ceux utilisés dans la littérature (on travaille avec des transformées de Fourier sur $257 pts$ pour chaque trame de $15ms$. La complexité des calculs est donc élevée). Nous allons développer par la suite les méthodes utilisées pour les différentes phases de l'algorithme de Détection d'activité vocale (on rappelle qu'il est composé d'un ensemble de sous-algorithmes).

\subsection{L'idée d'utiliser la distance entre des vecteurs de bruit de fond et celui de chaque trame ... } L'article de Hongzhi Wang et Yuchao Xu, Meijing Li propose d'extraire les Mel Features Cepstral Coefficients (MFCCs) d'un signal vocal pour chaque trame en considérant les dix (10) premières trames comme des trames de bruit d'arrière plan. En réalisant la moyenne de ces vecteurs, on obtient un vecteur de $MFCC_{bruité}$. Ce vecteur va permettre de réaliser la Corrélation des MFCC de chaque nouvelle trame avec lui même (dans l'article, ils utilisent aussi la distance euclidienne, qui se révèle être moins performante).Le vecteur de $MFCC_{bruité}$ est mis à jour à chaque trame :
\begin{equation}
	\underline{cno}=\underline{c}.p+(1-p).\underline{c}
\end{equation}
où $ \underline{cno}$ est le vecteur de $MFCC_{bruité}$, $c$ sont les coefficients de chaque nouvelle trame et $p$ est proche de 1. Le fait que $p$ doit-être proche de $1$ permet de garantir que le vecteur de MFCC bruité reste constitué de bruit de fond principalement malgré l'introduction de coefficients de parole à chaque nouvelle trame.
\paragraph{} Nous nous sommes donc appuyé de ces travaux pour construire notre algorithme de détection vocale. Nous avons aussi appliqué le principe de corrélation à l'entropie spectrale, toujours dans le soucis de rendre notre algorithme le plus robuste au bruit. 
\subsection{Détermination des seuils}
La détermination des seuils a été la partie la plus difficile à mettre en oeuvre, non pas que les algorithmes sont complexes, mais qu'une mauvaise méthode entraine la suppression des trames voulues. 
Ma première idée fut d'utiliser la distribution des valeurs obtenues. Cette distribution étant obtenue après avoir supprimé toutes les valeurs au-dessus de la moyenne du signal. Le but étant de récupérer la plus petites variations possibles du signal après analyse (Entropie et/ou MFCCs). Néanmoins, cette méthode bien qu'efficace était constante au court du temps ou du moins sur l'analyse d'un buffer d'échantillon. Il y avait donc des pertes et l'introduction de bruit après seuillage. 
La deuxième idée fut de trouver une méthode adaptée au chacun des paramètres utilisés. 
\subsubsection{Seuil pour les MFCCs}
Concernant les MFCCs, je me suis appuyé d'une fonction sigmoïde mise à jour à chaque fenêtre. La sigmoïde a été améliorée afin d'avoir pour $x=0$, une valeur proche de la valeur maximale du bruit (contrairement à la sigmoïde habituelle qui à pour valeur en $x=0$, 0,5). Cette valeur maximale du bruit est généralement proche de zéro de part la méthode utilisée pour déterminer la distance entre des trames de bruit de fond et la trame en cours (la distance d'un vecteur de bruit de fond à un autre vecteur de bruit donne généralement un résultat faible). 

\subsubsection{Seuil pour l'entropie spectrale}
La fonction de seuillage concernant l'entropie spectrale est quant à elle très standard du fait que les trames de parole sont très différentes de celle où il y a des silences ou du bruit. Pour cela, nous utilisons les 20 premières trames comme référence pour réaliser la valeur du seuil à l'instant $0$. On considère la moyenne de ces 20 trames et on y ajoutant trois (3) fois l'écart type de ces 20 trames :
\begin{equation}
	th_{noise}=\frac{1}{N}\sum_{k=1}^N x_k + 3 .\sqrt{\frac{1}{N-1}\sum_{i=1}^N|x_i -\mu|}
\end{equation}
où $\mu$ représente la moyenne du signal considéré. En effet, on considère que sur les 20 premières trames ne représentent que bruit. En y ajoutant trois fois l'écart-type, on s'assure d'être au-dessus du bruit de fond dans sa majeur partie.\\
Pour avoir un seuil pertinent et précis, on réalise une mise à jour ce seuil. La mise à jour est effective dès que l'on a 10 trames à traiter. En effet, nous n'avons pas de pertinence à utiliser la moyenne et l'écart type sur une trame. Dix trames nous semblaient convenable pour pouvoir exploiter ces deux indices pour une mise à jour correcte de notre seuil. Les étapes de la mise à jour sont les suivantes :\\
\begin{itemize}
	\item On collecte dix nouvelles trames
	\item On réalise l'opération suivante :
	\begin{equation}
		th_{new}=\frac{1}{N}\sum_{k=1}^N x_k + 3 .\sqrt{\frac{1}{N-1}\sum_{i=1}^N|x_i -\mu|}
	\end{equation}
	\item On met à jour la valeur du seuil pour les dix prochaines trames de la manière suivante (la mise à jour est similaire à celle du vecteur bruit pour l'entropie et les MFCCs) : 
	\begin{equation}
		th= p.th_{noise}+ (1-p)th_{new}
	\end{equation}
	Comme durant la mise à jour du vecteur bruit, on cherche à avoir une valeur de $p$ proche de $1$ (un) afin que la mise à jour ne soit pas polluée par une trame de voix. Cela rendrait nos seuil inutile car il ne considérerait plus les trames de voix ou alors celle avec des intensités plus élevées que celle considérée.
\end{itemize}
\paragraph{Amélioration possible du seuil} On pourrait, tout comme le seuil des MFCCs, considérer une fonction sigmoïde particulière pour calculer le seuil optimal à chaque trame. Pour le moment, celle-là n'a pas été développée car les résultats étaient satisfaisants. 
\subsubsection{Segmentation et correction des labels}
Nous en avions un peu parlé plus haut, une fois nos seuils identifiés, il faut labéliser chaque trame. Chaque label est soit un "1" soit un "0" ("0" correspondant à une trame sous le seuil). On réalise cette opération pour les deux paramètres que nous utilisions. Nous combinons nos résultats obtenus pour labéliser chaque trame (nous avons donc un "et" logique entre les deux vecteurs de labels obtenus). 
\paragraph{Correction des "uns" et "zéros" isolés} Notre vecteur de labels récupéré, ils arrivent que des "zéros" et des "uns" se retrouvent isolés dans une combinaison, respectivement, de "uns" et des "zéros". Afin de corriger ces "erreurs", on va chercher à évaluer le poids des labels entourant le label considéré. En soit, on balaye tous les labels afin de tous les vérifier. Les poids des labels entourant chaque label doit-être paire pour éviter des générer des nouvelles erreurs ou déplacer des labels (on a donc un nombre impaire des labels à évaluer pour trancher sur la valeur du label que l'on considère). On évalue donc le nombre de "un" et de "zéro", ce qui détermine si notre label est un "un" ou un "zéro". Mes propos ci-dessus n'étant pas forcément claire, illustrons mes propos avec un exemple :
\begin{enumerate}
	\item Considérons un vecteur de labels : $[ 1~1~0~0~1~1~1]$ et poids de 3 labels de part et d'autres du label que l'on évalue. On cherche à savoir si les "zéros" de ce vecteur de "uns" ne sont pas des erreurs. On se positionne sur le premier "zéro", on prend 3 labels à gauche et à droite (les premières valeurs n'ayant pas ou peu de labels sur leur gauche, on ne considère que le poids à d'autres en y intégrant au fur et à mesure les labels à gauche). On évalue donc le vecteur : $[ 1~1~{\color{red}0~0}~1~1]$. On constate qu'il y a 4 "un" pour 2 "zéro". Le label "zéro" considéré est en faite un "un" (car il y a plus de poids de "un" que de "zéro"). Finalement, en évaluant l'ensemble du vecteur de label, on a : $[ 1~1~{\color{red}1~1}~1~1~1]$. Notre vecteur est donc corrigé. 
\end{enumerate}
Aucune littérature n'a été utilisée pour traiter de ce problème, simplement des constats sur un grand nombre d'exemple. Le résultat obtenu est concluant, même si des améliorations sont tout à fait envisageable. Notamment sur le poids à considérer qui pourrait ne pas être fixe par exemple. On pourrait considérer que le poids devient plus grand quand, pendant un lapse de labels, ces derniers sont inchangeants. De fait, on pourrait corriger des "zéros"' ou "uns" isolés sur un plus grand poids. 
\paragraph{Segmentation}\footnote{L'idée s'appuit sur le principe des bufferisation que l'on utilise couramment en temps réel pour stocker nos données que l'on acquisitionne. Notamment en ce qui concerne les appelations "head" et "tail"}
Une fois le vecteur de labels corrigé, il nous reste plus qu'à extraire les segments où il y a de l'activité vocale. Pour cela, on considère un "head" et "tail". Le "head" caractérise le premier label "un" qui précède un "zéro" du vecteur et le "tail", le dernier "un" qui précède un groupe de "un" et est suivi d'un "zéro". Il peut y avoir plusieurs "head" et "tail" sur le signal acquisitionné. Le "head" est le premier marqueur, le "tail" lui n'apparait dès lors qu'un "head" est marqué sur le vecteur de labels. On extrait pour chaque "head" et "tail", leurs limites (la valeur de leur échantillon). Par exemple :
\begin{enumerate}
	\item On considère un vecteur de label : $[0~0~0~0~1~1~1~1~0~0~0]$ ({\bfseries{nb:}} la taille du vecteur est plus conséquente dans l'algorithme). On balaye toutes les valeurs du vecteur jusqu'au premier "head" (un "un" qui est précédé d'un "zéro"). On place donc le "head" sur le "un" qui représente le cinquième échantillon de notre vecteur ($[0~0~0~0~{\color{red}1}~1~1~1~0~0~0]$, ici en rouge). Puis on continue l'analyse mais cette fois-ci pour trouver le "tail". Ici le "tail" est le "un" qui représente le huitième échantillon du vecteur  ($[0~0~0~0~1~1~1~{\color{blue}1}~0~0~0]$, ici en bleu). Le "head" et le "tail" délimite donc le segment suivant : $[0~0~0~0~{\color{BurntOrange}1~1~1~1}~0~0~0]$.
\end{enumerate} 
Ensuite, le "head" et le "tail" renvoient leurs limites (à savoir la valeur de leur échantillon). On obtient donc :
	\[
\begin{bmatrix}
    Limits(head)    \\
    Limits(tail) \\
\end{bmatrix}
=
\begin{bmatrix}
    5 \\
    8 \\
\end{bmatrix}
\]
Pour extraire notre vecteur, il nous reste plus qu'à réaliser la conversion : Trame $\Leftrightarrow$ Samples. Pour cela, on utilise la formule suivante pour calculer l'échantillon du "head" : 
\begin{equation}
	Sample=Limits(head).step_{ms}
\end{equation}
où $step_{ms}$ représente la durée en $ms$ de la fenêtre d'étude en $ms$ moins le recouvrement que l'on considère en $ms$. Dans notre cas, elle est de $5ms$.
Pour calculer l'échantillon du "tail", l'opération est similaire, mais il faut veiller à rajouter la longueur en $ms$ de la fenêtre d'étude :
\begin{equation}
	Sample=Limits(head).step_{ms}+ window_{ms}
\end{equation}
où $window_{ms}$ est la fenêtre d'étude considérée. \\
\paragraph{} Les échantillons des "heads" et "tails" sont renvoyés dans un vecteur que l'on concatenate\footnote{Mots anglais : enchainement des vecteurs en soit} avec ceux déjà retournés. On obtient finalement une matrice. On n'a plus qu'à extraire les segments en s'appuyant de la matrice précédemment trouvée pour l'ensemble du signal.
\paragraph{Post traitement des segments}
Nous avions déjà commencé à développer l'idée un peu plus haut, nous allons continuer ici. Ce qu'il faut retenir c'est que malgré notre correction des labels un peu plus haut, des erreurs subsistent. Vous nous direz : "Mais alors quel fut l'avantage de corriger les labels un peu plus haut ?". Tout simplement, la fonction de correction des labels permet de corriger des labels isolés. Néanmoins, elle ne prend pas en compte le fait que les segments doivent-être d'une certaine taille pour pouvoir être assimilé à une activité vocale. En effet, la durée est quelque chose que nous n'avions pas évoqué, mais c'est un paramètre important à prendre en compte. Un phonème peut durer entre $20ms$ et $100ms$, une syllabe $100ms$ à minima. Aucune contrainte n'a été faite sur la longueur (en sample) de nos segments. Il se peut que ceux-ci ne durent qu'une centaine d'échantillons ($i.e$ moins de 5ms à $16~000Hz$). Il faut donc les rejeter car ne caractérise par une activité vocale. Pour cela, nous avons choisi de rejeter tous les segments représentants moins de $50ms$ en estimant qu'on ne peut avoir une activité vocale dans ce lapse de temps. Les autres segments sont quant à eux conservés. 
\subsection{Conclusion} Après avoir discuté des nos choix méthodiques et les algorithme mis en oeuvre, nous allons maintenant discuter des performances de notre algorithme de détection d'activité vocale en le comparant à des algorithmes. 
\newpage
\section{Tests et comparaisons}
Dans cette partie, nous allons comparer notre algorithme à ceux de la littérature. La plupart des algorithmes ont été récupérés sur la plate-forme collaborative de Matlab en se basant sur leur cotation mais aussi le respect de la littérature. 
\subsection{Présentation des algorithmes évalués}
Nous allons évaluer différents algorithmes à savoir :
\begin{itemize}
	\item Un algorithme basé sur la détection de l'énergie et du taux de passage par zéro. Les seuils sont fixés de manière manuelle dans une plage prédéfinie \cite{end}.
	\item Un algorithme basé sur la détection de l'énergie ainsi que le centroïde spectrale ("centre de gravité").\cite{silence} Les seuils sont définis par détection des maximas locaux de leur histogramme. Pour des explications un peu plus précise, nous vous invitons à lire son article\cite{centroid}.
	
\end{itemize}
\paragraph{} Plusieurs tests seront opérés :
\begin{itemize}
	\item Performances : Rapidité de la détection de l'algorithme.
	\item Robustesse au bruit : nous chercherons à ajouter progressivement du bruit dans nos tests  en baissant le SNR (Signal Noise Ratio\footnote{Taux du rapport signal à bruit}).
\end{itemize}
\paragraph{} Nous savons que nous partons dès le départ avec un désavantage. En effet, nous utilisons que des méthodes fréquentielles qui ont une complexité de calcul élevé. De plus, notre algorithme n'est pas optimisé car nous répétons certaines opérations plusieurs fois, notamment la plus gourmande, la Discrete Fourier Transform \footnote{Transformée de Fourier Discrète}. 
\subsubsection{Robustesse au bruit} 
Notre première tâche avant de parler de performances en terme de temps d'exécution est de tester leur robustesse face aux bruits. En d'autres termes, sachant que nous allons travailler dans des environnements différents, il faut veiller à ce que l'environnement de travail n'empiète pas sur la reconnaissance vocale. Pour cela, nous allons réaliser plusieurs test sur un fichier audio contenant de la voix. On veillera dans un premier à ce qu'il est pas ou peu de bruit de fond. Puis nous en rajouterons progressivement. Enfin, le test ultime sera dans un environnement musical, nous mettrons en fond, une trame musicale (disons d'un style "métal"\footnote{Le style métal ne représente par forcément le style englobant du morceau que nous choisirons. Néanmoins, nous vous donnons le style métal pour que vous puissiez vous représenter le bruit généré par ce style de musique}). Le but de chaque algorithme est sur cinq tests \footnote{Le nombre de tests est pris aléatoirement et de manière impaire} d'avoir le plus de trame avec présence vocale et le moins avec du bruit environnant. 
\paragraph{Données des tests}
Les données suivantes seront utilisées pour tous les test. En cas de changement, nous les rappelerons :
\begin{enumerate}
	\item Fréquence d'échantillonnage : $16~kHz$
	\item Quantification du signal : $16~bits$
	\item Taille des fenêtres : $15~ms$
	\item Décalage entre les fenêtres : $5~ms$
\end{enumerate}
\paragraph{Signal avec présence de voix et peu ou pas de bruit (SNR élévé)}
Pour commencer, nous allons illustrer le signal que nous allons utiliser. Il est assez simpliste, dure environ $5s$ et dans un premier temps, le SNR est très élevé. La figure ci-dessous (\figurename~\ref{figure_1}) est la représentation temporelle du signal avec des marqueurs indiquant la présence des mots "mode", "un et "deux". À titre d'information, le signal est échantillonné à $16kHz$ et quantitifé sur $16bits$. 
\begin{figure}[!h]
\centering
\includegraphics[width=15cm]{mode_un_deux_sb.eps}
\caption{Représentation temporelle du signal utilisé}
\label{figure_1}
\end{figure}
\paragraph{} Nous allons maintenant tester ce signal avec l'algorithme que nous proposons et ceux que nous avons soumis à titre comparatif. 
\subparagraph{Algorithme utilisant le ZRC (Zero Crossing Rate) et le STE( Short time Energy)}\footnote{ZRC : taux de passage par zéro ; STE :  énergie à court-terme}
\subparagraph{Algorithme utilisant le Spectral centroid et le STE( Short time Energy)}
Après avoir utilisé notre "morceau" en argument, nous obtenons le résultat suivant :
\begin{figure}[!h]
\centering
\includegraphics[width=10cm]{test_1_ec.eps}
\caption{Résultat de l'algorithme testé avec notre "morceau"}
\end{figure}
Les figures de l'algorithme n'étant pas forcément concrète sur le découpage (sauf si l'on sait que la partie grisée est la partie rejetée et les parties bordeaux sont celles conservées), nous avons utilisé les données retournées par l'algorithme pour vous présenter une figure similaire à ~\figurename~\ref{figure_1}.
\begin{figure}[!h]
\centering
\includegraphics[width=10cm]{test_1_2_ec.eps}
\caption{Résultat de l'algorithme testé avec notre "morceau"}
\end{figure}
\paragraph{} Etant donné que le découpage des segments n'était pas le même que celui présenté, nous avons labellisé les segments pour plus de compréhension. Comme vous pouvez le constater, le mot "mode" a été décomposé en syllabes. Ceci est dû au fenêtrage et au décalage très réduits.
\paragraph{Notre algorithme}
Nous avions évoqué un peu plus haut, le fait que nous utilisons un signal dit de bruit de fond. Dans notre cas, nous allons utiliser la fin du signal (où il n'y a aucune présence vocale à des fins de vecteur de bruit). Dans les conditions normales d'utilisation, ce dernier aurait été directement acquis en amont du traitement. 
\newpage
\clearpage
\bibliographystyle{plain} 
\bibliography{biblio.bib} 
\end{document}